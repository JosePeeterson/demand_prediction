{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepar.dataset.time_series import TimeSeriesTrain\n",
    "from deepar.model.learner import DeepARLearner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,sys\n",
    "#sys.path.append(os.path.abspath(os.path.join('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR')))\n",
    "sunspots_df = pd.read_csv('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR\\learningData.csv')\n",
    "# print(pd.to_timedelta(pd.to_datetime(sunspots_df['year'], format='%Y') ))\n",
    "# sunspots_df['year'] = pd.to_timedelta(pd.to_datetime(sunspots_df['year'], format='%Y') ).dt.total_seconds()\n",
    "#sunspots_ds_one = TimeSeriesTrain(sunspots_df, target_idx = 4, timestamp_idx = 1, index_col=0)\n",
    "\n",
    "#print(type(pd.to_datetime(sunspots_df['year'],format='%Y').values.astype('datetime64[D]')[0]))\n",
    "\n",
    "#sunspots_df['year'] = (pd.to_datetime(sunspots_df['year'], format='%Y') - pd.to_datetime('1818', format='%Y')).dt.total_seconds()\n",
    "#print(np.cumsum(sunspots_df['year']))\n",
    "\n",
    "\n",
    "sunspots_df['year'] = pd.to_datetime(np.cumsum(sunspots_df['year']), unit='s')\n",
    "#print(sunspots_df['year'])\n",
    "sunspots_ds_one = TimeSeriesTrain(sunspots_df, target_idx = 4, timestamp_idx = 1, index_col=0,count_data=True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR\\deepar\\deepar\\dataset\\time_series.py:247: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
      "  df['_week_of_year'] = df[self._time_name].dt.weekofyear\n",
      "c:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR\\deepar\\deepar\\dataset\\time_series.py:115: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cat_names = self._data[self._grouping_name].astype(str).append(pd.Series(['dummy_test_category']))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                   year   sd  observations  target   category  _hour_of_day  \\\n",
      "0   1970-01-01 00:00:01  9.2           213      53  dummy_cat             0   \n",
      "1   1970-01-01 00:00:02  7.9           249      39  dummy_cat             0   \n",
      "2   1970-01-01 00:00:03  6.4           224      24  dummy_cat             0   \n",
      "3   1970-01-01 00:00:04  4.2           304       9  dummy_cat             0   \n",
      "4   1970-01-01 00:00:05  3.7           353       6  dummy_cat             0   \n",
      "..                  ...  ...           ...     ...        ...           ...   \n",
      "196 1970-01-01 00:03:17  8.0          5273     113  dummy_cat             0   \n",
      "197 1970-01-01 00:03:18  6.4          8903      70  dummy_cat             0   \n",
      "198 1970-01-01 00:03:19  3.9          9940      40  dummy_cat             0   \n",
      "199 1970-01-01 00:03:20  2.5         11444      22  dummy_cat             0   \n",
      "200 1970-01-01 00:03:21  1.1         12250       7  dummy_cat             0   \n",
      "\n",
      "     _day_of_week  _day_of_month  _day_of_year  _week_of_year  _month_of_year  \\\n",
      "0               3              1             1              1               1   \n",
      "1               3              1             1              1               1   \n",
      "2               3              1             1              1               1   \n",
      "3               3              1             1              1               1   \n",
      "4               3              1             1              1               1   \n",
      "..            ...            ...           ...            ...             ...   \n",
      "196             3              1             1              1               1   \n",
      "197             3              1             1              1               1   \n",
      "198             3              1             1              1               1   \n",
      "199             3              1             1              1               1   \n",
      "200             3              1             1              1               1   \n",
      "\n",
      "     _year  _age  \n",
      "0     1970     0  \n",
      "1     1970     1  \n",
      "2     1970     2  \n",
      "3     1970     3  \n",
      "4     1970     4  \n",
      "..     ...   ...  \n",
      "196   1970   196  \n",
      "197   1970   197  \n",
      "198   1970   198  \n",
      "199   1970   199  \n",
      "200   1970   200  \n",
      "\n",
      "[201 rows x 13 columns]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                   year   sd  observations  target   category  _hour_of_day  \\\n",
      "0   1970-01-01 00:00:01  9.2           213      53  dummy_cat             0   \n",
      "1   1970-01-01 00:00:02  7.9           249      39  dummy_cat             0   \n",
      "2   1970-01-01 00:00:03  6.4           224      24  dummy_cat             0   \n",
      "3   1970-01-01 00:00:04  4.2           304       9  dummy_cat             0   \n",
      "4   1970-01-01 00:00:05  3.7           353       6  dummy_cat             0   \n",
      "..                  ...  ...           ...     ...        ...           ...   \n",
      "196 1970-01-01 00:03:17  8.0          5273     113  dummy_cat             0   \n",
      "197 1970-01-01 00:03:18  6.4          8903      70  dummy_cat             0   \n",
      "198 1970-01-01 00:03:19  3.9          9940      40  dummy_cat             0   \n",
      "199 1970-01-01 00:03:20  2.5         11444      22  dummy_cat             0   \n",
      "200 1970-01-01 00:03:21  1.1         12250       7  dummy_cat             0   \n",
      "\n",
      "     _day_of_week  _day_of_month  _day_of_year  _week_of_year  _month_of_year  \\\n",
      "0               3              1             1              1               1   \n",
      "1               3              1             1              1               1   \n",
      "2               3              1             1              1               1   \n",
      "3               3              1             1              1               1   \n",
      "4               3              1             1              1               1   \n",
      "..            ...            ...           ...            ...             ...   \n",
      "196             3              1             1              1               1   \n",
      "197             3              1             1              1               1   \n",
      "198             3              1             1              1               1   \n",
      "199             3              1             1              1               1   \n",
      "200             3              1             1              1               1   \n",
      "\n",
      "     _year  _age  \n",
      "0     1970     0  \n",
      "1     1970     1  \n",
      "2     1970     2  \n",
      "3     1970     3  \n",
      "4     1970     4  \n",
      "..     ...   ...  \n",
      "196   1970   196  \n",
      "197   1970   197  \n",
      "198   1970   198  \n",
      "199   1970   199  \n",
      "200   1970   200  \n",
      "\n",
      "[201 rows x 13 columns]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                   year   sd  observations  target   category  _hour_of_day  \\\n",
      "0   1970-01-01 00:00:01  9.2           213      53  dummy_cat             0   \n",
      "1   1970-01-01 00:00:02  7.9           249      39  dummy_cat             0   \n",
      "2   1970-01-01 00:00:03  6.4           224      24  dummy_cat             0   \n",
      "3   1970-01-01 00:00:04  4.2           304       9  dummy_cat             0   \n",
      "4   1970-01-01 00:00:05  3.7           353       6  dummy_cat             0   \n",
      "..                  ...  ...           ...     ...        ...           ...   \n",
      "196 1970-01-01 00:03:17  8.0          5273     113  dummy_cat             0   \n",
      "197 1970-01-01 00:03:18  6.4          8903      70  dummy_cat             0   \n",
      "198 1970-01-01 00:03:19  3.9          9940      40  dummy_cat             0   \n",
      "199 1970-01-01 00:03:20  2.5         11444      22  dummy_cat             0   \n",
      "200 1970-01-01 00:03:21  1.1         12250       7  dummy_cat             0   \n",
      "\n",
      "     _day_of_week  _day_of_month  _day_of_year  _week_of_year  _month_of_year  \\\n",
      "0               3              1             1              1               1   \n",
      "1               3              1             1              1               1   \n",
      "2               3              1             1              1               1   \n",
      "3               3              1             1              1               1   \n",
      "4               3              1             1              1               1   \n",
      "..            ...            ...           ...            ...             ...   \n",
      "196             3              1             1              1               1   \n",
      "197             3              1             1              1               1   \n",
      "198             3              1             1              1               1   \n",
      "199             3              1             1              1               1   \n",
      "200             3              1             1              1               1   \n",
      "\n",
      "     _year  _age  \n",
      "0     1970     0  \n",
      "1     1970     1  \n",
      "2     1970     2  \n",
      "3     1970     3  \n",
      "4     1970     4  \n",
      "..     ...   ...  \n",
      "196   1970   196  \n",
      "197   1970   197  \n",
      "198   1970   198  \n",
      "199   1970   199  \n",
      "200   1970   200  \n",
      "\n",
      "[201 rows x 13 columns]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'filepath' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR\\deepar\\Experimentation.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Work/WORK_PACKAGE/Demand_forecasting/github/DeepAR/deepar/Experimentation.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m sunspots_ds_one \u001b[39m=\u001b[39m TimeSeriesTrain(sunspots_df, target_idx \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m, timestamp_idx \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, index_col\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,count_data\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Work/WORK_PACKAGE/Demand_forecasting/github/DeepAR/deepar/Experimentation.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m learner \u001b[39m=\u001b[39m DeepARLearner(sunspots_ds_one, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Work/WORK_PACKAGE/Demand_forecasting/github/DeepAR/deepar/Experimentation.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m learner\u001b[39m.\u001b[39;49mfit(epochs \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, steps_per_epoch \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m, early_stopping \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,tensorboard\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,checkpoint_dir\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Work/WORK_PACKAGE/Demand_forecasting/github/DeepAR/deepar/Experimentation.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m test_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mWork\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mWORK_PACKAGE\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDemand_forecasting\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mgithub\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDeepAR\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtest_data\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mlearningData.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Work/WORK_PACKAGE/Demand_forecasting/github/DeepAR/deepar/Experimentation.ipynb#W4sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m test_df[\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(np\u001b[39m.\u001b[39mcumsum(test_df[\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m]), unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR\\deepar\\deepar\\model\\learner.py:431\u001b[0m, in \u001b[0;36mDeepARLearner.fit\u001b[1;34m(self, checkpoint_dir, validation, steps_per_epoch, epochs, early_stopping, stopping_patience, stopping_delta, tensorboard)\u001b[0m\n\u001b[0;32m    427\u001b[0m     val_gen \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \u001b[39m# Iterate over epochs.\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_training_loop(\n\u001b[1;32m--> 431\u001b[0m     filepath,\n\u001b[0;32m    432\u001b[0m     train_gen,\n\u001b[0;32m    433\u001b[0m     val_gen,\n\u001b[0;32m    434\u001b[0m     epochs\u001b[39m=\u001b[39mepochs,\n\u001b[0;32m    435\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39msteps_per_epoch,\n\u001b[0;32m    436\u001b[0m     early_stopping\u001b[39m=\u001b[39mearly_stopping,\n\u001b[0;32m    437\u001b[0m     stopping_patience\u001b[39m=\u001b[39mstopping_patience,\n\u001b[0;32m    438\u001b[0m     stopping_delta\u001b[39m=\u001b[39mstopping_delta,\n\u001b[0;32m    439\u001b[0m )\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'filepath' referenced before assignment"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from deepar.dataset.time_series import TimeSeriesTrain, TimeSeriesTest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deepar.model.learner import DeepARLearner\n",
    "\n",
    "sunspots_df = pd.read_csv('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR\\\\train_data\\learningData.csv')\n",
    "sunspots_df['year'] = pd.to_datetime(np.cumsum(sunspots_df['year']), unit='s')\n",
    "#print(sunspots_df['year'])\n",
    "sunspots_ds_one = TimeSeriesTrain(sunspots_df, target_idx = 4, timestamp_idx = 1, index_col=0,count_data=True)\n",
    "\n",
    "\n",
    "learner = DeepARLearner(sunspots_ds_one, verbose=1)\n",
    "\n",
    "learner.fit(epochs = 1, steps_per_epoch = 10, early_stopping = False,tensorboard=True,checkpoint_dir=None )\n",
    "\n",
    "\n",
    "test_df = pd.read_csv('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR\\\\test_data\\learningData.csv')\n",
    "\n",
    "test_df['year'] = pd.to_datetime(np.cumsum(test_df['year']), unit='s')\n",
    "\n",
    "test_ds = TimeSeriesTest(sunspots_ds_one,test_df,  target_idx = 4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preds = learner.predict(test_ds, horizon = None, samples = 1, include_all_training = True).reshape(-1)\n",
    "\n",
    "scores = pd.read_csv('../datasets/seed_datasets_current/56_sunspots/SCORE/dataset_SCORE/tables/learningData.csv')\n",
    "\n",
    "# evaluate compared to D3M test set\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "rms = sqrt(mean_squared_error(scores['sunspots'], preds))\n",
    "print(f'rms: {rms}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepar.dataset.time_series import TimeSeriesTrain, TimeSeriesTest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deepar.model.learner import DeepARLearner\n",
    "\n",
    "pop_df = pd.read_csv('../datasets/seed_datasets_current/LL1_736_population_spawn/TRAIN/dataset_TRAIN/tables/learningData.csv')\n",
    "pop_df = pop_df.set_index(['species', 'sector'])\n",
    "pop_df['group'] = pop_df.index\n",
    "pop_df['group'] = pop_df['group'].apply(lambda x: \" \".join(x))\n",
    "pop_df = pop_df.reset_index(drop=True)\n",
    "pop_df['day'] = pd.to_timedelta(pd.to_datetime(pop_df['day'], unit='D')).dt.total_seconds()\n",
    "pop_ds = TimeSeriesTrain(pop_df, target_idx = 2, timestamp_idx = 1, index_col=0, grouping_idx = 3, count_data = True)\n",
    "\n",
    "learner = DeepARLearner(pop_ds, verbose = 1)\n",
    "learner.fit(epochs = 1, batches = 10, early_stopping = False)\n",
    "\n",
    "test_df = pd.read_csv('../datasets/seed_datasets_current/LL1_736_population_spawn/TEST/dataset_TEST/tables/learningData.csv')\n",
    "test_df = test_df.set_index(['species', 'sector'])\n",
    "test_df['group'] = test_df.index\n",
    "test_df['group'] = test_df['group'].apply(lambda x: \" \".join(x))\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "test_df['day'] = pd.to_timedelta(pd.to_datetime(test_df['day'], unit='D')).dt.total_seconds()\n",
    "test_ds = TimeSeriesTest(test_df, pop_ds, target_idx = 2, timestamp_idx = 1, index_col=0, grouping_idx = 3)\n",
    "\n",
    "preds = learner.predict(test_ds, horizon = None, samples = 1, include_all_training = True)\n",
    "\n",
    "# evaluate compared to D3M test set\n",
    "scores = pd.read_csv('../datasets/seed_datasets_current/LL1_736_population_spawn/SCORE/dataset_SCORE/tables/learningData.csv')\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "print(scores['count'].shape)\n",
    "# rms = sqrt(mean_squared_error(scores['count'], preds))\n",
    "# print(f'rms: {rms}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from numpy.random import normal\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "window = 20\n",
    "features = 9\n",
    "def get_sample_prediction(sample, fn):\n",
    "    sample = np.array(sample).reshape(1, window, features)\n",
    "    output = fn([sample])\n",
    "    samples = []\n",
    "    for mu,sigma in zip(output[0].reshape(window), output[1].reshape(window)):\n",
    "        samples.append(normal(loc=mu, scale=np.sqrt(sigma), size=1)[0])\n",
    "    return np.array(samples)\n",
    "\n",
    "batch = sunspots_ds_one.next_batch(1, window)\n",
    "ress = []\n",
    "for i in range(300):\n",
    "    ress.append(get_sample_prediction(batch[0], dp_model.predict_theta_from_input))\n",
    "\n",
    "res_df = pd.DataFrame(ress).T\n",
    "tot_res = res_df\n",
    "\n",
    "\n",
    "plt.plot(batch[1].reshape(window), linewidth=6)\n",
    "tot_res['mu'] = tot_res.apply(lambda x: np.mean(x), axis=1)\n",
    "tot_res['upper'] = tot_res.apply(lambda x: np.mean(x) + np.std(x), axis=1)\n",
    "tot_res['lower'] = tot_res.apply(lambda x: np.mean(x) - np.std(x), axis=1)\n",
    "tot_res['two_upper'] = tot_res.apply(lambda x: np.mean(x) + 2*np.std(x), axis=1)\n",
    "tot_res['two_lower'] = tot_res.apply(lambda x: np.mean(x) - 2*np.std(x), axis=1)\n",
    "\n",
    "\n",
    "plt.plot(tot_res.mu, 'bo')\n",
    "plt.plot(tot_res.mu, linewidth=2)\n",
    "plt.fill_between(x = tot_res.index, y1=tot_res.lower, y2=tot_res.upper, alpha=0.5)\n",
    "plt.fill_between(x = tot_res.index, y1=tot_res.two_lower, y2=tot_res.two_upper, alpha=0.5)\n",
    "plt.title('Prediction uncertainty')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepar.model.loss import negative_binomial_likelihood\n",
    "from deepar.model.loss import gaussian_likelihood\n",
    "import tensorflow as tf\n",
    "\n",
    "sigma = 1.0\n",
    "sigma1 = 2.0\n",
    "sigma2 = 4.0\n",
    "g = gaussian_likelihood([sigma] * 3)\n",
    "b = negative_binomial_likelihood([sigma] * 3)\n",
    "b1 = negative_binomial_likelihood([sigma1] * 3)\n",
    "b2 = negative_binomial_likelihood([sigma2] * 3)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    y_true = tf.convert_to_tensor([1,1,1], dtype=tf.float32)\n",
    "    y_pred0 = tf.convert_to_tensor([1,1,1], dtype=tf.float32)\n",
    "    y_pred1 = tf.convert_to_tensor([2,2,2], dtype=tf.float32)\n",
    "    y_pred2 = tf.convert_to_tensor([4,4,4], dtype=tf.float32)\n",
    "    y_pred3 = tf.convert_to_tensor([8,8,8], dtype=tf.float32)\n",
    "    print(b(y_true, y_pred0).eval())\n",
    "    print(b(y_true, y_pred1).eval())\n",
    "    print(b(y_true, y_pred2).eval())\n",
    "    print(b(y_true, y_pred3).eval())\n",
    "    print(b1(y_true, y_pred0).eval())\n",
    "    print(b2(y_true, y_pred0).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.to_datetime(\"21/11/06 16:30\", format=\"%d/%m/%y %H:%M\")\n",
    "x.timetuple().tm_yday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('cat').apply(lambda x: x.count())['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('cat').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat, c_d in df.groupby('cat'):\n",
    "    c_d['age'] = c_d.index\n",
    "    print(c_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('age', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = df.groupby('cat').cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age1'] = range(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "df[[0,1]] = StandardScaler().fit_transform(df[[0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + df.groupby('cat')[0].agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf_handsOn_ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2991deb6652ffc93b150588010d8405bd17f8b0c61829990b75078345c7428a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
